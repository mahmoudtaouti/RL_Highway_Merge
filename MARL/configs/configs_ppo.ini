[MODEL_CONFIG]
; for the RMSprop optimizer
epsilon = 1e-5
alpha = 0.99
reward_gamma = 0.99
MAX_GRAD_NORM = 5
; roll out n steps
ROLL_OUT_N_STEPS = 100
; only remember the latest ROLL_OUT_N_STEPS
MEMORY_CAPACITY = 100
; only use the latest ROLL_OUT_N_STEPS for training A2C
BATCH_SIZE = 100
ENTROPY_REG = 0.01
; seeds for pytorch, 0, 2000, 2021
torch_seed = 0
TARGET_UPDATE_STEPS = 3
TARGET_TAU = 1.0

; concurrent
training_strategy = concurrent
actor_hidden_size = 128
critic_hidden_size = 128
action_masking = False
; "regionalR", "global_R"
reward_type = global_R

[TRAIN_CONFIG]
MAX_EPISODES = 20000
EPISODES_BEFORE_TRAIN = 1
EVAL_EPISODES = 3
EVAL_INTERVAL = 200
reward_scale = 20.
actor_lr = 5e-4
critic_lr = 5e-4
test_seeds = 0,25,50,75,100,125,150,175,200,325,350,375,400,425,450,475,500,525,550,575


[ENV_CONFIG]
; seed for the environment, 0, 2000, 2021
seed = 0
; [Hz]
simulation_frequency = 15
; time step
duration = 20
policy_frequency = 5
COLLISION_REWARD = 200
HIGH_SPEED_REWARD = 1
HEADWAY_COST = 4
HEADWAY_TIME = 1.2
MERGING_LANE_COST = 4
; 1: easy,  2: medium,  3: hard
traffic_density = 1
